{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a544b7b-bf71-4738-983a-56ed10e133eb",
   "metadata": {},
   "source": [
    "# Data Munging with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc95fc-c403-4556-8423-2ec83458f0db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installation etc.\n",
    "\n",
    "Look, I hate to be that guy, but Google/DDG are your friends here.  \n",
    "Getting ```Spark```, ```pySpark``` and other libraries to run is more than a little tedious.  \n",
    "The following are a 'best-guess' set of instrustions.  \n",
    "The ones that worked for me.  \n",
    "Using Windows 10 here.  \n",
    "Your mileage may vary.  \n",
    "\n",
    "Once you are through the tedium of installation and setup - it gets good. I promise. :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec543e-8ec3-4d72-9b03-9f4c62725514",
   "metadata": {},
   "source": [
    "## Download (and install where applicable)\n",
    "* JDK (prefer 8.x/11.x, 64bit, more open the better)\n",
    "* Hadoop (3.2.x, at this time) - _for windows, we just need Hadoop Winutils_\n",
    "* [Hadoop *winutils* (corresponding to the version of Hadoop)](https://github.com/cdarlint/winutils), [another repo](https://github.com/kontext-tech/winutils)\n",
    "* [Spark (3.x, at this time)](https://spark.apache.org/downloads.html)  \n",
    "* [Anaconda - Open Source/Individual Edition](https://www.anaconda.com/products/distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b7fb1-d67d-4943-829f-7e21af47502f",
   "metadata": {},
   "source": [
    "## Setup environment variables \n",
    "\n",
    "We set these environment variables that help manage paths better.\n",
    "Example variable values would look like:\n",
    "Java:  \n",
    "* JAVA_HOME = ```C:\\[Java]```  \n",
    "    \n",
    "Hadoop:  \n",
    "* HADOOP_HOME = ```C:\\hadoop\\hadoop-3.2.1```  \n",
    "_On Windows 10, we just need HADOOP_HOME to be the folder where **winutils.exe** is located_\n",
    "\n",
    "finally, Spark:   \n",
    "* SPARK_HOME = ```C:\\Spark\\spark-3.2.1-bin-hadoop3.2```  \n",
    "\n",
    "*notice there are no backslashes in the end. This is because slashes will be added in the next step when we setup path*      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7dbaa-30d1-41ba-8573-3470d74abd0e",
   "metadata": {},
   "source": [
    "## Update system **'PATH'**\n",
    "\n",
    "We use the variables defined above to set-up paths.  \n",
    "\n",
    "* Java: ```%JAVA_HOME%/bin```\n",
    "* Hadoop 01: ```%HADOOP_HOME%/bin```\n",
    "* Hadoop 02: ```%HADOOP_HOME%/sbin``` (*sbin needed in addition to bin*)\n",
    "* Spark: ```%SPARK_HOME%/bin```  \n",
    "    \n",
    "(*here we add backslashes before bin*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301fb83-c2ee-42a9-9c2c-57226377c48f",
   "metadata": {},
   "source": [
    "## Patch Hadoop  \n",
    "\n",
    "This is *needed* when Hadoop is run on Windows.\n",
    "\n",
    "* copy the ```bin``` folder from the right version of winutils to replace ```%HADOOP_HOME%/bin```  \n",
    "\n",
    "* copy ```hadoop-yarn-server-timelineservice-3.0.3``` from ```%HADOOP_HOME%\\share\\hadoop\\yarn\\timelineservice``` to ```%HADOOP_HOME%\\share\\hadoop\\yarn``` (the parent directory).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5cdca-66e7-4cb2-aacd-11d8379630c9",
   "metadata": {},
   "source": [
    "## Install the Python libraries\n",
    "\n",
    "Prefer installing [Anaconda](https://www.anaconda.com/products/distribution). \n",
    "It resolves other dependencies like Pandas, Numpy, Jupyter etc. too.  \n",
    "Once there, use either pip or conda - they are both cool but incompatible.  \n",
    "The conda-forge channel is a few days behind the pip one.  \n",
    "We're only running on the local machine here, no complicated infrastructure to care about.  \n",
    "So, you do you.  \n",
    "\n",
    "Use one of the following commands (from the command line obvs) to install each:  \n",
    "* pyspark:\n",
    "    * ```pip install pyspark``` or\n",
    "    * ```conda install -c conda-forge pyspark```\n",
    "* findspark:\n",
    "    * ```pip install findspark``` or\n",
    "    * ```conda install -c conda-forge findspark```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98fc72-dbd6-49db-afc9-9c16e10385d3",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "* [How to install Hadoop on Win 10](https://muhammadbilalyar.github.io/blogs/How-to-install-Hadoop-on-Window-10/)\n",
    "* [Hadoop on Windows](https://github.com/MuhammadBilalYar/Hadoop-On-Window)\n",
    "* [Hadoop and Spark on Windows](https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-windows-33kc)\n",
    "\n",
    "### (_Optionally_) Configure Hadoop\n",
    "\n",
    "*only needed if you want to use hadoop as your file storage system*  \n",
    "\n",
    "* create a folder for ```namenode```\n",
    "* create a folder for ```datanode```\n",
    "* four files: ```core-site.xml```, ```mapred-site.xml```, ```hdfs-site.xml```, ```yarn-site.xml``` - see code for each in the [reference repo](https://github.com/MuhammadBilalYar/Hadoop-On-Window) above.\n",
    "\n",
    "### Also,\n",
    "\n",
    "The scope of this notebook is *usage* - not setup or troubleshooting, am pretty sure these installation instructions will be outdated soon and be replaced by pre-built docker images or shell scripts or automated installs for windows or such-like.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cf8df-816d-4e9f-937d-f711263a2189",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This boiler plate helps, esp. in Jupyter Notebook situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7fca2-50cb-44d7-ae1e-e35ac201a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcf28b-084b-4227-8bb3-d7a714c9e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa73c1c-b624-478a-ab4c-2b43dc896a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a spark session\n",
    "\n",
    "# 'local[1]' indicates spark on 1 core on the local machine, specify the number of cores needed\n",
    "# use .config(\"spark.some.config.option\", \"some-value\") for additional configuration\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .appName(\"10+ minutes to pyspark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196ac0a-608f-4632-a456-d30b1ed57efc",
   "metadata": {},
   "source": [
    "Back in the day you'd need various 'contexts' as entry points into spark functionality.  \n",
    "All of this is now wrapped into a SparkSession, easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769ab4a-2aec-4324-9aec-c678e6f7929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SparkSession carries the sparkContext\n",
    "# spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b128f6-7001-4f98-8204-2e6ff76c4366",
   "metadata": {},
   "source": [
    "Check out the spark UI link when you uncomment the lines in the two cells above.  \n",
    "Your local UI should launch at a link like: http://localhost:4041/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fd9ee-5023-46ed-8c2c-202ba45e138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we close the notebook, stop spark, otherwise Jupyter closes, but scala-spark keep going on...\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596fcf-6641-4ccc-94fd-0cac6b4f28bf",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce9eb-a318-4782-b83b-a3e1d1b03ad4",
   "metadata": {},
   "source": [
    "## DataFrames: Create and View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8ace1-e5fe-473d-880f-dc41d2c2888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0c32e-9b00-42be-98b3-58759ab43864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of pyspark.sql.Row\n",
    "df1 = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1,b=2.,c='span a',d=date(2022,7,1),e=datetime(2022,7,1,12,0)),\n",
    "        Row(a=1,b=3.,c='can a ',d=date(2022,7,2),e=datetime(2022,7,2,12,0,1)),\n",
    "        Row(a=1,b=4.,c='banana',d=date(2022,7,3),e=datetime(2022,7,3,12,0,2))\n",
    "    ]\n",
    ")\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8125b-16ca-49b9-9625-d2223b0faabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1's not been evaluated yet. It's lazy evaluation\n",
    "# to eval it, we go\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bdf5d-6461-41a7-a207-68ebf1e409e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of tuples with explicit schema\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (2,5.,'man a',date(2022,7,1),datetime(2022,7,1,12,0)),\n",
    "        (2,6.,'can a',date(2022,8,1),datetime(2022,7,2,12,0)),\n",
    "        (2,7.,'manna',date(2022,9,1),datetime(2022,7,3,12,0))\n",
    "    ],\n",
    "    schema = 'a bigint, b double, c string, d date, e timestamp'\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba1bfe-4867-4ad1-8e33-acfa6b312b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spark RDDs to create a dataframe\n",
    "# go to the sparksession's sparkContext to access parallelize method\n",
    "rdd3 = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        (3,5., 'main', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'brain', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'pain', date(2022,7,1), datetime(2022,7,1,12,0,1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "df3 = spark.createDataFrame(rdd3, schema=['a','b','c','d','e'])\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e30524-a08e-4f72-ad29-6a61b1fe3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also use a pandas dataframe to create a spark dataframe\n",
    "df4_pd = pd.DataFrame(\n",
    "    {\n",
    "        'a': np.random.randint(0,10, size = 3),\n",
    "        'b': np.random.randn(3),\n",
    "        'c': [\"gandalf's manager\", \"said\", 'no'],\n",
    "        'd': [date(2022,7,1),date(2022,7,2),date(2022,7,3)],\n",
    "        'e': [datetime(2022,7,1,12,0,1),datetime(2022,7,2,12,0,2),datetime(2022,7,3,12,0,3)]\n",
    "    }\n",
    ")\n",
    "\n",
    "df4 = spark.createDataFrame(df4_pd)\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29122a68-ee0c-4d07-b6d3-c6900a5cd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bdf10-9527-4cad-8e71-1e2b4529c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use printSchema() to..., you know, it says what it does\n",
    "# also don't you hate that pySpark is following the Java camelCase instead of the Python snake_case?\n",
    "# yeah, what's that all about?\n",
    "\n",
    "df1.printSchema()\n",
    "df2.printSchema()\n",
    "df3.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33356611-4248-4825-8cff-69377bba13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show only x rows\n",
    "df1.show(1)\n",
    "# vertical - if the row is too long for horizontal display\n",
    "df4.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf225d7-710e-4e69-80f7-f63bd7a89147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect() - collects the entire df from across all nodes to the driver\n",
    "# if you don't have enough memory, here's how you crash spark\n",
    "# careful is the word\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d415b40-f85f-4a95-89de-189aa16e617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas used to have a take() method, deprecated now\n",
    "# take() in spark extracts the first n rows of a dataframe\n",
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d793a-a269-46c4-82ba-c3a9c5f4a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as take() but returns the last n rows of a dataframe\n",
    "df2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8be5f3-ca9e-4103-8ec4-bc7a9acc3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hey you want another way of crashing the spark driver?\n",
    "# convert the spark dataframe to a pandas dataframe\n",
    "# it'll collect all data from all workers into the driver\n",
    "df3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49288e-7455-4a84-b2bd-237ad913d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I make it so my dataframes are evaluated eagerly?\n",
    "# instead of the regular lazy eval?\n",
    "# y'know when am in notebooks and stuff?\n",
    "# set the config\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f9473-8440-417f-909b-dd7b6060fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there you go, eager evaulatio'\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446b176-8186-4aa7-bd72-e6e481b130fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it back to false if you like\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2725b11-35f6-488a-95a1-eef0b27cec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No more eagerly evaluated dataframes\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d15109-8688-4106-bb8f-e631b9b9f58a",
   "metadata": {},
   "source": [
    "# Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b63bf6-0858-44c2-b5f6-f76fec5dc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access a column\n",
    "df1.a, df2.b, df3.c, df4.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307d39a-b253-42fc-9891-1874efbff6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f2913-e512-40c1-b2db-592bbe923e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1.c) == type(upper(df1.c)) == type(df1.c.isNull())\n",
    "# TODO: what's going on with type(df1.c.isNull()) above???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea79385-3d12-4fd3-9b91-a69c0d068e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.c.isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefb474-fc0d-4d9c-b702-84c4e9baa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataframe's select method to identify a column and show() it\n",
    "df1.select(df1.c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe31305-cf4b-4956-8fe2-52381e13fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also there's dataframe.filter()\n",
    "df4.filter(df4.a>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989931c3-1191-4706-93e4-21f299b3ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c17a5d-ae8a-4c45-998f-0685497d5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a new column instance to the dataframe\n",
    "df4_withNewCol = df4.withColumn('upper_c', upper(df4.c))\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ee356-4dad-47a9-9cfb-900231fec55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7c395-3668-46e5-89ae-ab801c5009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "df4.filter(df4.a == 9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37002785-5d6f-4d06-aa0e-dfa309dc24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.filter(df4.b > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b895559-ebb0-49df-849e-2bc5d5b1839d",
   "metadata": {},
   "source": [
    "# UDFs: Applying a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705daf5-a919-4097-9280-a90630ec6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    #     plus one using pandas series\n",
    "    return series+1\n",
    "\n",
    "df4.select(df4.a, pandas_plus_one(df4.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d946c1-48cd-4693-a360-96bfb29ec43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_filter(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        yield pandas_df[pandas_df.b>0]\n",
    "        \n",
    "df4.mapInPandas(pandas_filter, schema = df4.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967daa8-65ea-4704-a097-32f22619c993",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Split-Apply-Combine, just like pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ba63a-e9db-477d-af55-bd6ffd955ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by fruit, color etc.\n",
    "df5 = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb849da3-352d-4ab6-aed7-bbd79307f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dd4ae-b463-45ff-a328-63c53a67748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How to get the deviations in a new column?\n",
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "df5.groupby('color').applyInPandas(plus_mean, schema = df5.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e1244-6157-4f61-b7c9-fabb8a423db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-grouping and applying a function.\n",
    "\n",
    "df6 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 1.0), \n",
    "        (20000101, 2, 2.0), \n",
    "        (20000102, 1, 3.0), \n",
    "        (20000102, 2, 4.0)\n",
    "    ],\n",
    "    ('time', 'id', 'v1')\n",
    ")\n",
    "\n",
    "df7 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 'x'), \n",
    "        (20000101, 2, 'y')\n",
    "    ],\n",
    "    ('time', 'id', 'v2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c472e-0e6e-45c0-83c4-be4a128fe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l,r,on='time', by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b10405-0406-4526-be30-a4df2b2cd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_gb = df6.groupby('id')\n",
    "df7_gb = df7.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0171df-3121-4a56-b78d-f20ef56fcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_grp = df6_gb.cogroup(df7_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2ec3d-0699-4536-b048-a2f61be71fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = co_grp.applyInPandas(asof_join, schema='time int, id int, v1 double, v2 string')\n",
    "rslt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9eb347-0c6b-4c7a-b95e-03f8e930e220",
   "metadata": {},
   "source": [
    "# Getting data in and out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746de44d-7c20-456d-bee9-ad657f12b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df5.write.csv('fruits.csv', header=True)\n",
    "spark.read.csv('fruits.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2869caf-bb97-4d65-a439-abcd54b387b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "df5.write.parquet('fruits.parquet')\n",
    "spark.read.parquet('fruits.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bf0e5-675b-47de-aaff-30b8dda710ba",
   "metadata": {},
   "source": [
    "# Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f76a79-4196-40a0-8911-2b16b90fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.createOrReplaceTempView('tableA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf4ac5-9523-4837-bc7c-276ccd963317",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(*) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10234261-9c90-47cb-8f8f-bc6410cb67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs in SQL\n",
    "# register and invoke\n",
    "@pandas_udf('integer')\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s+1\n",
    "\n",
    "spark.udf.register('add_one', add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d62221-e6d4-43dc-8864-455f64824782",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT v1, add_one(v1) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c181f2-978c-44af-a436-08e6098b01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can mix/match sql expressions \n",
    "# for e.g. take the expressions from above\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df5.selectExpr('add_one(v1)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55466bb4-3b91-4d75-9848-937ec8020bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75739657-6e2e-4a58-b33d-43e0f3588cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')>0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77513663-399f-400a-8be1-c66945461b82",
   "metadata": {},
   "source": [
    "# Pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26873600-5e9e-4d32-813b-50a136f9256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097116d-aaea-404c-9911-0187a766bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas on spark series\n",
    "\n",
    "s1 = ps.Series([1,2,3,np.nan,5,np.nan,7,8,9])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d032e-2d95-44ac-8df1-fe22957896af",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1 = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
    "    },\n",
    "    index=[10, 20, 30, 40, 50, 60]\n",
    ")\n",
    "\n",
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdb212-22f8-41f3-b328-71784cb501f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe and convert to pandas-spark\n",
    "dates1 = pd.date_range('20220101', periods=6)\n",
    "pdf1 = pd.DataFrame(np.random.randn(6,4), index=dates1, columns = list('ABCD'))\n",
    "# create spark dataframe from the pandas dataframe\n",
    "psdf1 = ps.from_pandas(pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c202f-86a7-497c-b3ea-6d7101cef227",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b5c19-d254-47b1-8db9-3469a0d7c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ping pong between spark dataframe and pandas on spark dataframe\n",
    "# effectively you can create one from the other and vice versa\n",
    "\n",
    "sdf1 = spark.createDataFrame(psdf1)\n",
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124fe81-2591-4177-bfb1-4ccdbd0dd217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b4b6c-103c-4c90-95c1-5ff34214bcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303255-1f39-42e8-8d76-c9ba054dff9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f80c80-1f13-450c-8c5d-0f7f3ac3d181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
