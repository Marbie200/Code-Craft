{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a544b7b-bf71-4738-983a-56ed10e133eb",
   "metadata": {},
   "source": [
    "# Data Munging with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427649f",
   "metadata": {},
   "source": [
    "### <font color='green'>_This notebook supports Google Colab_  </font>\n",
    "\n",
    "<font color='green'>Look for the \"_Sidebar_: Google Colab\" section below to setup and run this Spark notebook on Google Colab.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc95fc-c403-4556-8423-2ec83458f0db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installation etc.\n",
    "\n",
    "Look, I hate to be that guy, but Google/DDG are your friends here.  \n",
    "Getting ```Spark```, ```pySpark``` and other libraries to run is more than a little tedious.  \n",
    "The following are a 'best-guess' set of instrustions.  \n",
    "The ones that worked for me.  \n",
    "Using Windows 10 here.  \n",
    "Your mileage may vary.  \n",
    "\n",
    "Once you are through the tedium of installation and setup - it gets good. I promise. :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec543e-8ec3-4d72-9b03-9f4c62725514",
   "metadata": {},
   "source": [
    "## Download (and install where applicable)\n",
    "* JDK (prefer 8.x/11.x, 64bit, more open the better)\n",
    "* Hadoop (3.2.x, at this time) - _for windows, we just need Hadoop Winutils_\n",
    "* [Hadoop *winutils* (corresponding to the version of Hadoop)](https://github.com/cdarlint/winutils), [another repo](https://github.com/kontext-tech/winutils)\n",
    "* [Spark (3.x, at this time)](https://spark.apache.org/downloads.html)  \n",
    "* [Anaconda - Open Source/Individual Edition](https://www.anaconda.com/products/distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b7fb1-d67d-4943-829f-7e21af47502f",
   "metadata": {},
   "source": [
    "## Setup environment variables \n",
    "\n",
    "We set these environment variables that help manage paths better.\n",
    "Example variable values would look like:\n",
    "Java:  \n",
    "* JAVA_HOME = ```C:\\[Java]```  \n",
    "    \n",
    "Hadoop:  \n",
    "* HADOOP_HOME = ```C:\\hadoop\\hadoop-3.4.0-win10-x64```  \n",
    "_this is just a sample_  \n",
    "_On Windows, we need HADOOP_HOME to be the folder where **winutils.exe** is located_  \n",
    "_For dev purposes: you can just download winutils and move on, just ensure you point to winutils.exe in this env variable_\n",
    "\n",
    "finally, Spark:   \n",
    "* SPARK_HOME = ```C:\\Spark\\spark-3.4.1-bin-hadoop3```  \n",
    "\n",
    "*notice there are no backslashes in the end. This is because slashes will be added in the next step when we setup path*      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7dbaa-30d1-41ba-8573-3470d74abd0e",
   "metadata": {},
   "source": [
    "## Update system **'PATH'**\n",
    "\n",
    "We use the variables defined above to set-up paths.  \n",
    "\n",
    "* Java: ```%JAVA_HOME%/bin```\n",
    "* Hadoop 01: ```%HADOOP_HOME%/bin``` \n",
    "* Hadoop 02: ```%HADOOP_HOME%/sbin``` (*sbin needed in addition to bin*)\n",
    "* Spark: ```%SPARK_HOME%/bin```  \n",
    "    \n",
    "(*here we add backslashes before bin*)\n",
    "\n",
    "N.B.: If you are only doing the dev setup (just winutils and nothing else) thing, you may not need the Hadoop 01/02 above, I added those to my system anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301fb83-c2ee-42a9-9c2c-57226377c48f",
   "metadata": {},
   "source": [
    "## Patch Hadoop  \n",
    "\n",
    "This is *needed* when Hadoop is run on Windows.\n",
    "\n",
    "* copy the ```bin``` folder from the right version of winutils to replace ```%HADOOP_HOME%/bin```  \n",
    "\n",
    "* copy ```hadoop-yarn-server-timelineservice-3.0.3``` from ```%HADOOP_HOME%\\share\\hadoop\\yarn\\timelineservice``` to ```%HADOOP_HOME%\\share\\hadoop\\yarn``` (the parent directory).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5cdca-66e7-4cb2-aacd-11d8379630c9",
   "metadata": {},
   "source": [
    "## Install the Python libraries\n",
    "\n",
    "Prefer installing [Anaconda](https://www.anaconda.com/products/distribution). \n",
    "It resolves other dependencies like Pandas, Numpy, Jupyter etc. too.  \n",
    "Once there, use either pip or conda - they are both cool but incompatible.  \n",
    "The conda-forge channel is a few days behind the pip one.  \n",
    "We're only running on the local machine here, no complicated infrastructure to care about.  \n",
    "So, you do you.  \n",
    "\n",
    "Use one of the following commands (from the command line obvs) to install each:  \n",
    "* pyspark:\n",
    "    * ```pip install pyspark``` or\n",
    "    * ```conda install -c conda-forge pyspark```\n",
    "* findspark:\n",
    "    * ```pip install findspark``` or\n",
    "    * ```conda install -c conda-forge findspark```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98fc72-dbd6-49db-afc9-9c16e10385d3",
   "metadata": {},
   "source": [
    "## References  \n",
    "\n",
    "* [How to install Hadoop on Win 10](https://muhammadbilalyar.github.io/blogs/How-to-install-Hadoop-on-Window-10/)\n",
    "* [Hadoop on Windows](https://github.com/MuhammadBilalYar/Hadoop-On-Window)\n",
    "* [Hadoop and Spark on Windows](https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-windows-33kc)\n",
    "\n",
    "### (_Optionally_) Configure Hadoop\n",
    "\n",
    "*only needed if you want to use hadoop as your file storage system*  \n",
    "\n",
    "* create a folder for ```namenode```\n",
    "* create a folder for ```datanode```\n",
    "* four files: ```core-site.xml```, ```mapred-site.xml```, ```hdfs-site.xml```, ```yarn-site.xml``` - see code for each in the [reference repo](https://github.com/MuhammadBilalYar/Hadoop-On-Window) above.\n",
    "\n",
    "### Also,\n",
    "\n",
    "The scope of this notebook is *usage* - not setup or troubleshooting, am pretty sure these installation instructions will be outdated soon and be replaced by pre-built docker images or shell scripts or automated installs for windows or such-like.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cf8df-816d-4e9f-937d-f711263a2189",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This boiler plate helps, esp. in Jupyter Notebook situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61211d",
   "metadata": {},
   "source": [
    "## _Sidebar_: Google Colab\n",
    "\n",
    "You don't need to run this on your local machine.\n",
    "The notebook is setup to run on Google Colab as well.\n",
    "\n",
    "For a detailed description of how this is setup, see the [02.000 (optional) Setup_Spark_in_Google_Colab](https://github.com/shauryashaurya/learn-data-munging/blob/main/02.000%20(optional)%20Setup_Spark_in_Google_Colab.ipynb) notebook\n",
    "\n",
    "Open the notebook in Google Colab using the following button, then uncomment the setup marked # SETUP FOR COLAB  \n",
    "  \n",
    "  \n",
    "<a href=\"https://colab.research.google.com/github/shauryashaurya/learn-data-munging/blob/main/02.001%20-%2010%2B%20minutes%20to%20pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    \n",
    "\n",
    "_NOTE: keep the # SETUP FOR COLAB step below commented (disabled) when you are running this notebook locally_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b538d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP FOR COLAB: select all the lines below and uncomment (CTRL+/ on windows)\n",
    "\n",
    "# # grab spark\n",
    "# # as of Dec 2022, the latest version is 3.2.3, get the link from Apache Spark's website\n",
    "# ! wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
    "# # unzip spark\n",
    "# !tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
    "# # install findspark package\n",
    "# !pip install -q findspark\n",
    "\n",
    "# # got to provide JAVA_HOME and SPARK_HOME vairables\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d7fca2-50cb-44d7-ae1e-e35ac201a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffcf28b-084b-4227-8bb3-d7a714c9e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: import pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec09686-86ce-46a6-bc8f-ad2801530277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa73c1c-b624-478a-ab4c-2b43dc896a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a spark session\n",
    "\n",
    "# 'local[1]' indicates spark on 1 core on the local machine, specify the number of cores needed\n",
    "# use .config(\"spark.some.config.option\", \"some-value\") for additional configuration\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local[1]') \\\n",
    "    .appName(\"10+ minutes to pyspark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196ac0a-608f-4632-a456-d30b1ed57efc",
   "metadata": {},
   "source": [
    "Back in the day you'd need various 'contexts' as entry points into spark functionality.  \n",
    "All of this is now wrapped into a SparkSession, easy to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769ab4a-2aec-4324-9aec-c678e6f7929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SparkSession carries the sparkContext\n",
    "# spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b128f6-7001-4f98-8204-2e6ff76c4366",
   "metadata": {},
   "source": [
    "Check out the spark UI link when you uncomment the lines in the two cells above.  \n",
    "Your local UI should launch at a link like: http://localhost:4041/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8fd9ee-5023-46ed-8c2c-202ba45e138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we close the notebook, stop spark, otherwise Jupyter closes, but scala-spark keep going on...\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596fcf-6641-4ccc-94fd-0cac6b4f28bf",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ce9eb-a318-4782-b83b-a3e1d1b03ad4",
   "metadata": {},
   "source": [
    "## DataFrames: Create and View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f8ace1-e5fe-473d-880f-dc41d2c2888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a0c32e-9b00-42be-98b3-58759ab43864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of pyspark.sql.Row\n",
    "df1 = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1,b=2.,c='span a',d=date(2022,7,1),e=datetime(2022,7,1,12,0)),\n",
    "        Row(a=1,b=3.,c='can a ',d=date(2022,7,2),e=datetime(2022,7,2,12,0,1)),\n",
    "        Row(a=1,b=4.,c='banana',d=date(2022,7,3),e=datetime(2022,7,3,12,0,2))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51454297-dd70-4126-8502-ff56068ff68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b8125b-16ca-49b9-9625-d2223b0faabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|span a|2022-07-01|2022-07-01 12:00:00|\n",
      "|  1|3.0|can a |2022-07-02|2022-07-02 12:00:01|\n",
      "|  1|4.0|banana|2022-07-03|2022-07-03 12:00:02|\n",
      "+---+---+------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df1's not been evaluated yet. It's lazy evaluation\n",
    "# to eval it, we go\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019bdf5d-6461-41a7-a207-68ebf1e409e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a list of tuples with explicit schema\n",
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (2,5.,'man a',date(2022,7,1),datetime(2022,7,1,12,0)),\n",
    "        (2,6.,'can a',date(2022,8,1),datetime(2022,7,2,12,0)),\n",
    "        (2,7.,'manna',date(2022,9,1),datetime(2022,7,3,12,0))\n",
    "    ],\n",
    "    schema = 'a bigint, b double, c string, d date, e timestamp'\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ba1bfe-4867-4ad1-8e33-acfa6b312b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use spark RDDs to create a dataframe\n",
    "# go to the sparksession's sparkContext to access parallelize method\n",
    "rdd3 = spark.sparkContext.parallelize(\n",
    "    [\n",
    "        (3,5., 'main', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'brain', date(2022,7,1), datetime(2022,7,1,12,0,1)),\n",
    "        (3,5., 'pain', date(2022,7,1), datetime(2022,7,1,12,0,1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "df3 = spark.createDataFrame(rdd3, schema=['a','b','c','d','e'])\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e30524-a08e-4f72-ad29-6a61b1fe3236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+----------+-------------------+\n",
      "|  a|                 b|                c|         d|                  e|\n",
      "+---+------------------+-----------------+----------+-------------------+\n",
      "|  0|1.3461612779974976|gandalf's manager|2022-07-01|2022-07-01 12:00:01|\n",
      "|  3|0.8235447997632228|             said|2022-07-02|2022-07-02 12:00:02|\n",
      "|  3|1.1436223517625508|               no|2022-07-03|2022-07-03 12:00:03|\n",
      "+---+------------------+-----------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can also use a pandas dataframe to create a spark dataframe\n",
    "df4_pd = pd.DataFrame(\n",
    "    {\n",
    "        'a': np.random.randint(0,10, size = 3),\n",
    "        'b': np.random.randn(3),\n",
    "        'c': [\"gandalf's manager\", \"said\", 'no'],\n",
    "        'd': [date(2022,7,1),date(2022,7,2),date(2022,7,3)],\n",
    "        'e': [datetime(2022,7,1,12,0,1),datetime(2022,7,2,12,0,2),datetime(2022,7,3,12,0,3)]\n",
    "    }\n",
    ")\n",
    "\n",
    "df4 = spark.createDataFrame(df4_pd)\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29122a68-ee0c-4d07-b6d3-c6900a5cd713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.346161</td>\n",
       "      <td>gandalf's manager</td>\n",
       "      <td>2022-07-01</td>\n",
       "      <td>2022-07-01 12:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.823545</td>\n",
       "      <td>said</td>\n",
       "      <td>2022-07-02</td>\n",
       "      <td>2022-07-02 12:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.143622</td>\n",
       "      <td>no</td>\n",
       "      <td>2022-07-03</td>\n",
       "      <td>2022-07-03 12:00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a         b                  c           d                   e\n",
       "0  0  1.346161  gandalf's manager  2022-07-01 2022-07-01 12:00:01\n",
       "1  3  0.823545               said  2022-07-02 2022-07-02 12:00:02\n",
       "2  3  1.143622                 no  2022-07-03 2022-07-03 12:00:03"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6bdf10-9527-4cad-8e71-1e2b4529c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use printSchema() to..., you know, it says what it does\n",
    "# also don't you hate that pySpark is following the Java camelCase instead of the Python snake_case?\n",
    "# yeah, what's that all about?\n",
    "\n",
    "df1.printSchema()\n",
    "df2.printSchema()\n",
    "df3.printSchema()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33356611-4248-4825-8cff-69377bba13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+----------+-------------------+\n",
      "|  a|  b|     c|         d|                  e|\n",
      "+---+---+------+----------+-------------------+\n",
      "|  1|2.0|span a|2022-07-01|2022-07-01 12:00:00|\n",
      "+---+---+------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0------------------\n",
      " a   | 0                   \n",
      " b   | 1.3461612779974976  \n",
      " c   | gandalf's manager   \n",
      " d   | 2022-07-01          \n",
      " e   | 2022-07-01 12:00:01 \n",
      "-RECORD 1------------------\n",
      " a   | 3                   \n",
      " b   | 0.8235447997632228  \n",
      " c   | said                \n",
      " d   | 2022-07-02          \n",
      " e   | 2022-07-02 12:00:02 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show only x rows\n",
    "df1.show(1)\n",
    "# vertical - if the row is too long for horizontal display\n",
    "df4.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf225d7-710e-4e69-80f7-f63bd7a89147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=3, b=5.0, c='main', d=datetime.date(2022, 7, 1), e=datetime.datetime(2022, 7, 1, 12, 0, 1)),\n",
       " Row(a=3, b=5.0, c='brain', d=datetime.date(2022, 7, 1), e=datetime.datetime(2022, 7, 1, 12, 0, 1)),\n",
       " Row(a=3, b=5.0, c='pain', d=datetime.date(2022, 7, 1), e=datetime.datetime(2022, 7, 1, 12, 0, 1))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect() - collects the entire df from across all nodes to the driver\n",
    "# if you don't have enough memory, here's how you crash spark\n",
    "# careful is the word\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d415b40-f85f-4a95-89de-189aa16e617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=2, b=5.0, c='man a', d=datetime.date(2022, 7, 1), e=datetime.datetime(2022, 7, 1, 12, 0)),\n",
       " Row(a=2, b=6.0, c='can a', d=datetime.date(2022, 8, 1), e=datetime.datetime(2022, 7, 2, 12, 0))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas used to have a take() method, deprecated now\n",
    "# take() in spark extracts the first n rows of a dataframe\n",
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f0d793a-a269-46c4-82ba-c3a9c5f4a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=2, b=6.0, c='can a', d=datetime.date(2022, 8, 1), e=datetime.datetime(2022, 7, 2, 12, 0)),\n",
       " Row(a=2, b=7.0, c='manna', d=datetime.date(2022, 9, 1), e=datetime.datetime(2022, 7, 3, 12, 0))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same as take() but returns the last n rows of a dataframe\n",
    "df2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df8be5f3-ca9e-4103-8ec4-bc7a9acc3099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>main</td>\n",
       "      <td>2022-07-01 00:00:00</td>\n",
       "      <td>2022-07-01 12:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>brain</td>\n",
       "      <td>2022-07-01 00:00:00</td>\n",
       "      <td>2022-07-01 12:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pain</td>\n",
       "      <td>2022-07-01 00:00:00</td>\n",
       "      <td>2022-07-01 12:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b      c                    d                    e\n",
       "0  3  5.0   main  2022-07-01 00:00:00  2022-07-01 12:00:01\n",
       "1  3  5.0  brain  2022-07-01 00:00:00  2022-07-01 12:00:01\n",
       "2  3  5.0   pain  2022-07-01 00:00:00  2022-07-01 12:00:01"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hey you want another way of crashing the spark driver?\n",
    "# convert the spark dataframe to a pandas dataframe\n",
    "# it'll collect all data from all workers into the driver\n",
    "\n",
    "# this naive approach will fail because we have date-time values in df3 . \n",
    "# df3.toPandas()\n",
    "# possible error: TypeError: Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.\n",
    "\n",
    "# explicitly provide date-time data type to Pandas\n",
    "from pyspark.sql.functions import date_format\n",
    "df3.withColumn(\"d\", date_format(\"d\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"e\", date_format(\"e\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49288e-7455-4a84-b2bd-237ad913d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do I make it so my dataframes are evaluated eagerly?\n",
    "# instead of the regular lazy eval?\n",
    "# y'know when am in notebooks and stuff?\n",
    "# set the config\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f9473-8440-417f-909b-dd7b6060fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there you go, eager evaulatio'\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446b176-8186-4aa7-bd72-e6e481b130fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it back to false if you like\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2725b11-35f6-488a-95a1-eef0b27cec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No more eagerly evaluated dataframes\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d15109-8688-4106-bb8f-e631b9b9f58a",
   "metadata": {},
   "source": [
    "# Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b63bf6-0858-44c2-b5f6-f76fec5dc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access a column\n",
    "df1.a, df2.b, df3.c, df4.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307d39a-b253-42fc-9891-1874efbff6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "from pyspark.sql.functions import upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f2913-e512-40c1-b2db-592bbe923e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df1.c) == type(upper(df1.c)) == type(df1.c.isNull())\n",
    "# TODO: what's going on with type(df1.c.isNull()) above???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea79385-3d12-4fd3-9b91-a69c0d068e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.c.isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefb474-fc0d-4d9c-b702-84c4e9baa02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dataframe's select method to identify a column and show() it\n",
    "df1.select(df1.c).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe31305-cf4b-4956-8fe2-52381e13fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also there's dataframe.filter()\n",
    "df4.filter(df4.a>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989931c3-1191-4706-93e4-21f299b3ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c17a5d-ae8a-4c45-998f-0685497d5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a new column instance to the dataframe\n",
    "df4_withNewCol = df4.withColumn('upper_c', upper(df4.c))\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ee356-4dad-47a9-9cfb-900231fec55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.show()\n",
    "df4_withNewCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7c395-3668-46e5-89ae-ab801c5009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "df4.filter(df4.a == 9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37002785-5d6f-4d06-aa0e-dfa309dc24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.filter(df4.b > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b895559-ebb0-49df-849e-2bc5d5b1839d",
   "metadata": {},
   "source": [
    "# UDFs: Applying a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705daf5-a919-4097-9280-a90630ec6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    #     plus one using pandas series\n",
    "    return series+1\n",
    "\n",
    "df4.select(df4.a, pandas_plus_one(df4.a)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d946c1-48cd-4693-a360-96bfb29ec43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_filter(iterator):\n",
    "    for pandas_df in iterator:\n",
    "        yield pandas_df[pandas_df.b>0]\n",
    "        \n",
    "df4.mapInPandas(pandas_filter, schema = df4.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d967daa8-65ea-4704-a097-32f22619c993",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Split-Apply-Combine, just like pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ba63a-e9db-477d-af55-bd6ffd955ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by fruit, color etc.\n",
    "df5 = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb849da3-352d-4ab6-aed7-bbd79307f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dd4ae-b463-45ff-a328-63c53a67748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How to get the deviations in a new column?\n",
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "df5.groupby('color').applyInPandas(plus_mean, schema = df5.schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e1244-6157-4f61-b7c9-fabb8a423db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-grouping and applying a function.\n",
    "\n",
    "df6 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 1.0), \n",
    "        (20000101, 2, 2.0), \n",
    "        (20000102, 1, 3.0), \n",
    "        (20000102, 2, 4.0)\n",
    "    ],\n",
    "    ('time', 'id', 'v1')\n",
    ")\n",
    "\n",
    "df7 = spark.createDataFrame(\n",
    "    [\n",
    "        (20000101, 1, 'x'), \n",
    "        (20000101, 2, 'y')\n",
    "    ],\n",
    "    ('time', 'id', 'v2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c472e-0e6e-45c0-83c4-be4a128fe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l,r,on='time', by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b10405-0406-4526-be30-a4df2b2cd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6_gb = df6.groupby('id')\n",
    "df7_gb = df7.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0171df-3121-4a56-b78d-f20ef56fcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_grp = df6_gb.cogroup(df7_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2ec3d-0699-4536-b048-a2f61be71fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = co_grp.applyInPandas(asof_join, schema='time int, id int, v1 double, v2 string')\n",
    "rslt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9eb347-0c6b-4c7a-b95e-03f8e930e220",
   "metadata": {},
   "source": [
    "# Getting data in and out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746de44d-7c20-456d-bee9-ad657f12b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "# .mode('overwrite') can be taken out when needed.\n",
    "df5.write.mode('overwrite').csv('fruits.csv', header=True)\n",
    "spark.read.csv('fruits.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2869caf-bb97-4d65-a439-abcd54b387b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "# .mode('overwrite') can be taken out when needed.\n",
    "df5.write.mode('overwrite').parquet('fruits.parquet')\n",
    "spark.read.parquet('fruits.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bf0e5-675b-47de-aaff-30b8dda710ba",
   "metadata": {},
   "source": [
    "# Working with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f76a79-4196-40a0-8911-2b16b90fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.createOrReplaceTempView('tableA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf4ac5-9523-4837-bc7c-276ccd963317",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(*) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10234261-9c90-47cb-8f8f-bc6410cb67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs in SQL\n",
    "# register and invoke\n",
    "@pandas_udf('integer')\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s+1\n",
    "\n",
    "spark.udf.register('add_one', add_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d62221-e6d4-43dc-8864-455f64824782",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT v1, add_one(v1) from tableA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c181f2-978c-44af-a436-08e6098b01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can mix/match sql expressions \n",
    "# for e.g. take the expressions from above\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df5.selectExpr('add_one(v1)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55466bb4-3b91-4d75-9848-937ec8020bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75739657-6e2e-4a58-b33d-43e0f3588cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.select(expr('count(*)')>0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77513663-399f-400a-8be1-c66945461b82",
   "metadata": {},
   "source": [
    "# Pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26873600-5e9e-4d32-813b-50a136f9256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097116d-aaea-404c-9911-0187a766bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas on spark series\n",
    "\n",
    "s1 = ps.Series([1,2,3,np.nan,5,np.nan,7,8,9])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d032e-2d95-44ac-8df1-fe22957896af",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1 = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]\n",
    "    },\n",
    "    index=[10, 20, 30, 40, 50, 60]\n",
    ")\n",
    "\n",
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdb212-22f8-41f3-b328-71784cb501f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas dataframe and convert to pandas-spark\n",
    "dates1 = pd.date_range('20220101', periods=6)\n",
    "pdf1 = pd.DataFrame(np.random.randn(6,4), index=dates1, columns = list('ABCD'))\n",
    "# create spark dataframe from the pandas dataframe\n",
    "psdf1 = ps.from_pandas(pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c202f-86a7-497c-b3ea-6d7101cef227",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b5c19-d254-47b1-8db9-3469a0d7c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SPARK df from a PANDAS df (we've done this before)\n",
    "# create a PANDAS-ON-SPARK df from a SPARK df (we've done this too...)\n",
    "\n",
    "sdf2 = spark.createDataFrame(pdf1)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124fe81-2591-4177-bfb1-4ccdbd0dd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2 = sdf2.pandas_api()\n",
    "# this works like a pandas df now...\n",
    "psdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b4b6c-103c-4c90-95c1-5ff34214bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303255-1f39-42e8-8d76-c9ba054dff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas head()\n",
    "psdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f80c80-1f13-450c-8c5d-0f7f3ac3d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836234ff-538c-44e1-ac85-ec0eb74c26dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81bd98-1c33-4916-9457-f269ec38f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8614d3b-cbb0-4c62-9a6f-6aa0f7bc639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning - this breaks the driver due to memory - just like collect()\n",
    "# be careful\n",
    "psdf2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2d82e-ed61-47b9-9bc3-26bfc5d669ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose\n",
    "psdf2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff44ab7-bbe1-47d6-851c-785c410ec46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort index\n",
    "psdf2.sort_index(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24421bcc-fc85-45e7-a815-8240ffe9549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values\n",
    "psdf2.sort_values(by='B', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c44fe2-a0f9-4689-b6d3-cb8de4fff888",
   "metadata": {},
   "source": [
    "# Missing Data - Pandas on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec03db1-6e6f-4be8-9edf-0070d6003588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll notice we are repeating a lot of the tasks \n",
    "# from 10 minutes to pandas \n",
    "pdf2 = pdf1.reindex(index=dates1[0:4], columns=list(pdf1.columns) + ['E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e3352-2c85-4ae1-8d56-6feb75edcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.loc[dates1[0]:dates1[1], 'E'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b228f0-e92b-4850-a179-ac448fb8cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3 = ps.from_pandas(pdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429fe43-77a7-407c-85f0-1b418d26d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fa301-49c1-4d09-ad8c-affd32a61c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452aba8-c98e-447b-bae4-78ce734da322",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf3.fillna(value=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b0d8f-527e-4dfe-9c22-19530bc145e9",
   "metadata": {},
   "source": [
    "# Operations - Pandas on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6b507-a0cb-41c5-b046-7a36cd3f12f9",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915984d-a0c2-4464-8b08-3e82a082148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb20ff2-42a4-4c7f-8ecd-18c9fceac842",
   "metadata": {},
   "source": [
    "## Spark Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cf57d-e6eb-4b26-883d-9049eb9e6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the current config value\n",
    "previous = spark.conf.get('spark.sql.execution.arrow.pyspark.enabled')\n",
    "previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146679e-e0e4-4684-aae8-692f230f6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default index prevent overhead.\n",
    "ps.set_option('compute.default_index_type', 'distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8d4ab-b220-4d70-bce5-a1bb1c9c4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings coming from Arrow optimizations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216906a-96da-4fc8-a83b-b70d14e1ec55",
   "metadata": {},
   "source": [
    "Let's toggle ```spark.sql.execution.arrow.pyspark.enabled``` to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612acea-78d0-4eb6-8422-0c2babca0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is supposed to be faster by an order of magnitude\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256c0f7-1a31-4fea-a870-4d8e8d661e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ps.range(3000000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395df2b-a90c-44d2-8719-b31905891e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is supposed to be slower\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bce5b7-7596-44fa-bfb7-45ab35ac63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ps.range(3000000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f263b6e-8197-4a45-adda-0b715ad8f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything back as it was\n",
    "ps.reset_option('compute.default_index_type')\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c33947-af1f-4b21-9a7e-216e183b89b5",
   "metadata": {},
   "source": [
    "# Grouping  \n",
    "  \n",
    "  \n",
    "It's the same split-apply-combine deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2936f-f187-422f-905d-a91de2f2b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4 = ps.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                          'foo', 'bar', 'foo', 'foo'],\n",
    "                    'B': ['one', 'one', 'two', 'three',\n",
    "                          'two', 'two', 'one', 'three'],\n",
    "                    'C': np.random.randn(8),\n",
    "                    'D': np.random.randn(8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36143034-9676-4c05-9783-a47aeff13621",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da96c5-5e44-4f75-ad0f-dd6af42b27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.groupby('A').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897d230-59bb-466d-881d-95d010ba7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46f6d8-b844-4eb4-acf8-b5d8df5ff6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pser = pd.Series(np.random.randn(1000),\n",
    "                 index=pd.date_range('1/1/2000', periods=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dbf50-4f76-4543-903c-abc5e430de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser = ps.Series(pser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4bb833-7560-4d22-96b0-a620bef82e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser = psser.cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4177316-770e-48fc-a5c5-541bac983080",
   "metadata": {},
   "outputs": [],
   "source": [
    "psser.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b146fa9-b56c-437a-9c8e-06065022af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf4 = pd.DataFrame(np.random.randn(1000, 4), index=pser.index,\n",
    "                   columns=['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45332cad-eecf-4e6d-b191-87daef2696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5 = ps.from_pandas(pdf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787939c-912c-4169-894c-1a6417f5aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5 = psdf5.cummax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92f20f-bad4-45bc-9bb1-4458c114c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795f3c1-66da-4753-b91b-82d793ea926e",
   "metadata": {},
   "source": [
    "# Getting data in/out - Pandas on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d86fa-7907-40b6-981f-574422e3bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.to_csv('./data/foo.csv')\n",
    "ps.read_csv('./data/foo.csv').head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a9f5e-0c8f-40ce-bf6c-b6b0a576c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf4.to_parquet('./data/bar.parquet')\n",
    "ps.read_parquet('./data/bar.parquet').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e871f9-293f-4a90-84c5-bb934bc1e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f12fd2-370a-41a0-b98c-c70a9298a007",
   "metadata": {},
   "source": [
    "# Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb84fc-f294-42f8-b08e-80ff2ce2503f",
   "metadata": {},
   "source": [
    "Check out the exercises on real-world datasets next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
